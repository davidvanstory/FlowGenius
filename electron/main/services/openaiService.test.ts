/**
 * OpenAI Service Unit Tests
 * 
 * Comprehensive test suite for the OpenAI GPT-4 service covering:
 * - Service initialization and configuration
 * - Validation analysis functionality
 * - Question generation functionality
 * - Error handling and retry logic
 * - Rate limiting and API interaction
 * - Factory function creation
 */

import { describe, it, expect, beforeEach, afterEach, vi } from 'vitest';
import { OpenAIService, createOpenAIService } from './openaiService';
import type { 
  OpenAIConfig, 
  ValidationAnalysisRequest, 
  QuestionGenerationRequest,
  GPTRequestOptions
} from './openaiService';

// Mock the logger
vi.mock('../../../src/utils/logger', () => ({
  logger: {
    info: vi.fn(),
    error: vi.fn(),
    warn: vi.fn(),
    debug: vi.fn()
  }
}));

// Mock fetch globally
const mockFetch = vi.fn();
global.fetch = mockFetch;

// Test configuration
const validConfig: OpenAIConfig = {
  apiKey: 'sk-test123456789abcdef',
  baseUrl: 'https://api.openai.com/v1',
  timeout: 30000,
  maxRetries: 2,
  retryDelay: 1000,
  model: 'gpt-4o'
};

// Mock successful GPT-4 response
const mockGPTResponse = {
  choices: [
    {
      message: {
        content: JSON.stringify({
          addressedCriteria: ['problem_definition', 'target_audience'],
          completionScores: {
            'problem_definition': 0.8,
            'target_audience': 0.6
          },
          overallConfidence: 0.7,
          reasoning: 'User has clearly articulated the problem and identified potential users.'
        })
      },
      finish_reason: 'stop'
    }
  ],
  usage: {
    prompt_tokens: 150,
    completion_tokens: 80,
    total_tokens: 230
  },
  model: 'gpt-4o'
};

// Mock successful partial answer follow-up response
const mockPartialFollowupResponse = {
  choices: [
    {
      message: {
        content: JSON.stringify({
          followupQuestion: 'Yoda Says: You mentioned busy professionals - what specific industries or job roles would benefit most from your solution?',
          shouldContinueProbing: true,
          reasoning: 'Yoda Says: The user provided a general target audience but lacked specific industry context which is crucial for market validation.',
          expectedInformation: ['specific industries', 'job roles', 'professional context']
        })
      },
      finish_reason: 'stop'
    }
  ],
  usage: {
    prompt_tokens: 150,
    completion_tokens: 75,
    total_tokens: 225
  },
  model: 'gpt-4o'
};

// Mock successful question generation response
const mockQuestionResponse = {
  choices: [
    {
      message: {
        content: JSON.stringify({
          questions: [
            'Yoda Says: What specific pain points do your target users experience with current solutions?',
            'Yoda Says: How would you prioritize the key features based on user feedback?'
          ],
          reasoning: 'These questions focus on understanding user needs and feature prioritization. User needs assessment is highest priority as it drives all feature decisions.',
          prioritizedCriteria: ['user_pain_points', 'key_features'],
          priorityScores: {
            'user_pain_points': 9,
            'key_features': 7
          }
        })
      },
      finish_reason: 'stop'
    }
  ],
  usage: {
    prompt_tokens: 180,
    completion_tokens: 60,
    total_tokens: 240
  },
  model: 'gpt-4o'
};

// Mock successful validation analysis response
const mockAnalysisResponse = {
  choices: [
    {
      message: {
        content: JSON.stringify({
          addressedCriteria: ['problem_definition'],
          completionScores: { 'problem_definition': 0.85 },
          partialCriteria: ['target_audience'],
          partialScores: { 'target_audience': 0.6 },
          overallConfidence: 0.8,
          reasoning: 'Yoda Says: The user has clearly defined the problem but provided only general information about target audience.'
        })
      },
      finish_reason: 'stop'
    }
  ],
  usage: {
    prompt_tokens: 160,
    completion_tokens: 70,
    total_tokens: 230
  },
  model: 'gpt-4o'
};

describe('OpenAIService', () => {
  let openaiService: OpenAIService;

  beforeEach(() => {
    console.log('ðŸ§ª Setting up OpenAI service test');
    vi.clearAllMocks();
    mockFetch.mockClear();
    openaiService = new OpenAIService(validConfig);
  });

  afterEach(() => {
    vi.clearAllTimers();
  });

  describe('Service Initialization', () => {
    it('should initialize with valid configuration', () => {
      console.log('ðŸ§ª Testing valid service initialization');
      
      expect(openaiService).toBeInstanceOf(OpenAIService);
      
      const status = openaiService.getStatus();
      expect(status.config.baseUrl).toBe(validConfig.baseUrl);
      expect(status.config.timeout).toBe(validConfig.timeout);
      expect(status.config.maxRetries).toBe(validConfig.maxRetries);
      expect(status.config.hasApiKey).toBe(true);
      expect(status.config.model).toBe(validConfig.model);
      expect(status.requestCount).toBe(0);
    });

    it('should throw error without API key', () => {
      console.log('ðŸ§ª Testing initialization without API key');
      
      expect(() => {
        new OpenAIService({ apiKey: '' });
      }).toThrow('OpenAI API key is required for OpenAIService');
    });

    it('should use default configuration values', () => {
      console.log('ðŸ§ª Testing default configuration values');
      
      const minimalConfig = { apiKey: 'sk-test123' };
      const service = new OpenAIService(minimalConfig);
      
      const status = service.getStatus();
      expect(status.config.baseUrl).toBe('https://api.openai.com/v1');
      expect(status.config.timeout).toBe(45000);
      expect(status.config.maxRetries).toBe(3);
      expect(status.config.model).toBe('gpt-4o');
    });
  });

  describe('Validation Analysis', () => {
    const mockRequest: ValidationAnalysisRequest = {
      userResponse: 'I want to build an app that helps people find restaurants.',
      checklistCriteria: [
        'problem_definition',
        'target_audience',
        'user_pain_points',
        'key_features'
      ],
      conversationContext: ['Previous conversation about restaurant discovery'],
      operationId: 'test_validation_123'
    };

    it('should successfully analyze validation', async () => {
      console.log('ðŸ§ª Testing successful validation analysis');
      
      mockFetch.mockResolvedValueOnce({
        ok: true,
        json: () => Promise.resolve(mockAnalysisResponse)
      });

      const result = await openaiService.analyzeValidation(mockRequest);

      expect(result.success).toBe(true);
      expect(result.data).toBeDefined();
      expect(result.data?.addressedCriteria).toEqual(['problem_definition']);
      expect(result.data?.completionScores['problem_definition']).toBe(0.85);
      expect(result.data?.partialCriteria).toEqual(['target_audience']);
      expect(result.data?.partialScores['target_audience']).toBe(0.6);
      expect(result.data?.overallConfidence).toBe(0.8);
      expect(result.data?.reasoning).toContain('Yoda Says:');
      expect(result.operationId).toBe('test_validation_123');

      // Verify API call was made correctly
      expect(mockFetch).toHaveBeenCalledWith(
        'https://api.openai.com/v1/chat/completions',
        expect.objectContaining({
          method: 'POST',
          headers: expect.objectContaining({
            'Authorization': 'Bearer sk-test123456789abcdef',
            'Content-Type': 'application/json'
          }),
          body: expect.stringContaining('gpt-4o')
        })
      );
    });

    it('should handle API error responses', async () => {
      console.log('ðŸ§ª Testing API error handling');
      
      // Create a properly structured mock error response
      const mockErrorResponse = {
        ok: false,
        status: 400,
        statusText: 'Bad Request',
        json: vi.fn().mockResolvedValue({ 
          error: { 
            message: 'Invalid request parameters',
            type: 'invalid_request_error'
          }
        })
      };

      mockFetch.mockResolvedValue(mockErrorResponse);

      const result = await openaiService.analyzeValidation(mockRequest);

      expect(result.success).toBe(false);
      expect(result.error).toContain('400');
      expect(result.error).toContain('Bad Request');
      expect(result.data).toBeUndefined();
      expect(mockErrorResponse.json).toHaveBeenCalled();
    });

    it('should handle JSON parsing errors', async () => {
      console.log('ðŸ§ª Testing JSON parsing error handling');
      
      const invalidJsonResponse = {
        ...mockGPTResponse,
        choices: [
          {
            message: {
              content: 'Invalid JSON response'
            },
            finish_reason: 'stop'
          }
        ]
      };

      mockFetch.mockResolvedValueOnce({
        ok: true,
        json: () => Promise.resolve(invalidJsonResponse)
      });

      const result = await openaiService.analyzeValidation(mockRequest);

      expect(result.success).toBe(false);
      expect(result.error).toContain('Invalid JSON response from GPT-4 validation analysis');
    });

    it('should retry on retryable errors', async () => {
      console.log('ðŸ§ª Testing retry logic on retryable errors');
      
      // Use a shorter retry delay for faster testing
      const fastRetryService = new OpenAIService({
        ...validConfig,
        retryDelay: 10, // Very short delay for testing
        maxRetries: 2
      });

      // First call fails with retryable error
      const mockRetryableErrorResponse = {
        ok: false,
        status: 503,
        statusText: 'Service Unavailable',
        json: vi.fn().mockResolvedValue({ 
          error: { 
            message: 'Service temporarily unavailable',
            type: 'server_error'
          }
        })
      };

      // Second call succeeds
      const mockSuccessResponse = {
        ok: true,
        json: vi.fn().mockResolvedValue(mockAnalysisResponse)
      };

      mockFetch
        .mockResolvedValueOnce(mockRetryableErrorResponse)
        .mockResolvedValueOnce(mockSuccessResponse);

      const result = await fastRetryService.analyzeValidation(mockRequest);

      expect(result.success).toBe(true);
      expect(result.retryCount).toBe(1);
      expect(mockFetch).toHaveBeenCalledTimes(2);
      expect(mockRetryableErrorResponse.json).toHaveBeenCalled();
      expect(mockSuccessResponse.json).toHaveBeenCalled();
    });

    it('should not retry on authentication errors', async () => {
      console.log('ðŸ§ª Testing no retry on authentication errors');
      
      mockFetch.mockRejectedValueOnce(new Error('Unauthorized - invalid api key'));

      const result = await openaiService.analyzeValidation(mockRequest);

      expect(result.success).toBe(false);
      expect(result.error).toContain('invalid api key');
      expect(mockFetch).toHaveBeenCalledTimes(1); // No retry
    });

    it('should handle missing choices in API response', async () => {
      console.log('ðŸ§ª Testing missing choices handling');
      
      const noChoicesResponse = {
        choices: [], // Empty choices array
        usage: {
          prompt_tokens: 10,
          completion_tokens: 0,
          total_tokens: 10
        },
        model: 'gpt-4o'
      };

      const mockNoChoicesResponse = {
        ok: true,
        json: vi.fn().mockResolvedValue(noChoicesResponse)
      };

      mockFetch.mockResolvedValue(mockNoChoicesResponse);

      const result = await openaiService.analyzeValidation(mockRequest);

      expect(result.success).toBe(false);
      expect(result.error).toContain('No choices returned from GPT-4 API');
      expect(mockNoChoicesResponse.json).toHaveBeenCalled();
    });
  });

  describe('Question Generation', () => {
    const mockRequest: QuestionGenerationRequest = {
      userResponse: 'I described my restaurant app idea.',
      checklistCriteria: [
        'problem_definition',
        'target_audience',
        'user_pain_points',
        'key_features'
      ],
      unaddressedCriteria: ['user_pain_points', 'key_features'],
      conversationContext: ['Previous conversation'],
      maxQuestions: 2,
      operationId: 'test_questions_456'
    };

    it('should successfully generate questions', async () => {
      console.log('ðŸ§ª Testing successful question generation');
      
      mockFetch.mockResolvedValueOnce({
        ok: true,
        json: () => Promise.resolve(mockQuestionResponse)
      });

      const result = await openaiService.generateQuestions(mockRequest);

      expect(result.success).toBe(true);
      expect(result.data).toBeDefined();
      expect(result.data?.questions).toHaveLength(2);
      expect(result.data?.questions[0]).toContain('Yoda Says:');
      expect(result.data?.questions[0]).toContain('pain points');
      expect(result.data?.prioritizedCriteria).toContain('user_pain_points');
      expect(result.data?.priorityScores).toBeDefined();
      expect(result.data?.priorityScores['user_pain_points']).toBe(9);
      expect(result.data?.priorityScores['key_features']).toBe(7);
      expect(result.operationId).toBe('test_questions_456');
    });

    it('should limit questions to max specified', async () => {
      console.log('ðŸ§ª Testing question count limiting');
      
      const manyQuestionsResponse = {
        ...mockQuestionResponse,
        choices: [
          {
            message: {
              content: JSON.stringify({
                questions: [
                  'Yoda Says: Question 1?',
                  'Yoda Says: Question 2?',
                  'Yoda Says: Question 3?',
                  'Yoda Says: Question 4?',
                  'Yoda Says: Question 5?'
                ],
                reasoning: 'Many questions generated for testing prioritization.',
                prioritizedCriteria: ['criteria1', 'criteria2', 'criteria3', 'criteria4', 'criteria5'],
                priorityScores: {
                  'criteria1': 10,
                  'criteria2': 8,
                  'criteria3': 6,
                  'criteria4': 4,
                  'criteria5': 2
                }
              })
            },
            finish_reason: 'stop'
          }
        ]
      };

      mockFetch.mockResolvedValueOnce({
        ok: true,
        json: () => Promise.resolve(manyQuestionsResponse)
      });

      const result = await openaiService.generateQuestions({
        ...mockRequest,
        maxQuestions: 3
      });

      expect(result.success).toBe(true);
      expect(result.data?.questions).toHaveLength(3); // Limited to max
    });

    it('should handle network timeout', async () => {
      console.log('ðŸ§ª Testing network timeout handling');
      
      // Mock timeout error that would be thrown by AbortSignal.timeout
      const timeoutError = new Error('The operation was aborted due to timeout');
      timeoutError.name = 'AbortError';
      
      mockFetch.mockRejectedValue(timeoutError);

      const result = await openaiService.generateQuestions(mockRequest);

      expect(result.success).toBe(false);
      expect(result.error).toContain('operation was aborted');
    });
  });

  describe('Rate Limiting and Performance', () => {
    it('should enforce rate limiting between requests', async () => {
      console.log('ðŸ§ª Testing rate limiting');
      
      const fastService = new OpenAIService({
        ...validConfig,
        maxRetries: 1 // Reduce retries for faster test
      });

      mockFetch.mockResolvedValue({
        ok: true,
        json: () => Promise.resolve(mockGPTResponse)
      });

      const startTime = Date.now();
      
      // Make two quick requests
      const request: ValidationAnalysisRequest = {
        userResponse: 'Test',
        checklistCriteria: ['test']
      };

      await fastService.analyzeValidation(request);
      await fastService.analyzeValidation(request);

      const endTime = Date.now();
      const duration = endTime - startTime;

      // Should have some delay due to rate limiting
      expect(duration).toBeGreaterThan(200); // At least 200ms delay
    });

    it('should track request statistics', async () => {
      console.log('ðŸ§ª Testing request statistics tracking');
      
      mockFetch.mockResolvedValue({
        ok: true,
        json: () => Promise.resolve(mockGPTResponse)
      });

      const initialStatus = openaiService.getStatus();
      expect(initialStatus.requestCount).toBe(0);

      await openaiService.analyzeValidation({
        userResponse: 'Test',
        checklistCriteria: ['test']
      });

      const updatedStatus = openaiService.getStatus();
      expect(updatedStatus.requestCount).toBe(1);
      expect(updatedStatus.lastRequestTime).toBeGreaterThan(0);
    });
  });

  describe('createOpenAIService Factory Function', () => {
    beforeEach(() => {
      // Clear environment variables
      delete process.env.OPENAI_API_KEY;
      delete process.env.OPENAI_BASE_URL;
      delete process.env.OPENAI_TIMEOUT;
      delete process.env.OPENAI_MAX_RETRIES;
      delete process.env.OPENAI_RETRY_DELAY;
      delete process.env.OPENAI_ORGANIZATION;
      delete process.env.OPENAI_MODEL;
    });

    it('should create service from environment variables', () => {
      console.log('ðŸ§ª Testing factory function with environment variables');
      
      process.env.OPENAI_API_KEY = 'sk-env-api-key';
      process.env.OPENAI_TIMEOUT = '60000';
      process.env.OPENAI_MAX_RETRIES = '5';
      process.env.OPENAI_MODEL = 'gpt-4';

      const service = createOpenAIService();
      const status = service.getStatus();

      expect(status.config.hasApiKey).toBe(true);
      expect(status.config.timeout).toBe(60000);
      expect(status.config.maxRetries).toBe(5);
      expect(status.config.model).toBe('gpt-4');
    });

    it('should throw error without OPENAI_API_KEY', () => {
      console.log('ðŸ§ª Testing factory function without API key');
      
      expect(() => {
        createOpenAIService();
      }).toThrow('OPENAI_API_KEY environment variable is required');
    });

    it('should use default values for optional env vars', () => {
      console.log('ðŸ§ª Testing factory function with minimal environment');
      
      process.env.OPENAI_API_KEY = 'sk-minimal-key';

      const service = createOpenAIService();
      const status = service.getStatus();

      expect(status.config.hasApiKey).toBe(true);
      expect(status.config.baseUrl).toBe('https://api.openai.com/v1');
      expect(status.config.timeout).toBe(45000);
      expect(status.config.maxRetries).toBe(3);
      expect(status.config.model).toBe('gpt-4o');
    });
  });

  describe('Error Handling Edge Cases', () => {
    it('should handle quota exceeded errors without retry', async () => {
      console.log('ðŸ§ª Testing quota exceeded error handling');
      
      mockFetch.mockRejectedValueOnce(new Error('Quota exceeded - billing issue'));

      const result = await openaiService.analyzeValidation({
        userResponse: 'Test',
        checklistCriteria: ['test']
      });

      expect(result.success).toBe(false);
      expect(result.error).toContain('billing issue');
      expect(mockFetch).toHaveBeenCalledTimes(1); // No retry
    });

    it('should handle very long conversation context', async () => {
      console.log('ðŸ§ª Testing long conversation context handling');
      
      const longContext = Array(20).fill('Long conversation message');
      
      mockFetch.mockResolvedValueOnce({
        ok: true,
        json: () => Promise.resolve(mockGPTResponse)
      });

      const result = await openaiService.analyzeValidation({
        userResponse: 'Test with long context',
        checklistCriteria: ['test'],
        conversationContext: longContext
      });

      expect(result.success).toBe(true);
      
      // Verify the request body contains truncated context (last 5 messages)
      const callBody = JSON.parse(mockFetch.mock.calls[0][1].body);
      const systemMessage = callBody.messages.find(m => m.role === 'system');
      expect(systemMessage).toBeDefined();
    });
  });

  describe('generateDynamicChecklist', () => {
    const mockDynamicChecklistResponse = {
      choices: [
        {
          message: {
            content: JSON.stringify({
              generatedCriteria: [
                {
                  id: 'user_onboarding_flow',
                  question: 'How will you onboard new users to your meditation app?',
                  priority: 5,
                  reasoning: 'Onboarding is critical for app adoption'
                },
                {
                  id: 'meditation_personalization',
                  question: 'How will you personalize meditation experiences for different stress levels?',
                  priority: 4,
                  reasoning: 'Personalization drives engagement'
                }
              ],
              reasoning: 'These criteria focus on user experience and personalization for a meditation app',
              productType: 'Consumer Mobile App - Wellness',
              targetAudience: 'Busy professionals with high stress levels'
            })
          },
          finish_reason: 'stop'
        }
      ],
      usage: { prompt_tokens: 200, completion_tokens: 150, total_tokens: 350 },
      model: 'gpt-4o'
    };

    it('should generate tailored checklist criteria successfully', async () => {
      console.log('ðŸ§ª Testing dynamic checklist generation');

      mockFetch.mockResolvedValueOnce({
        ok: true,
        json: () => Promise.resolve(mockDynamicChecklistResponse)
      });

      const request = {
        userIdea: 'I want to build a meditation app for busy professionals who struggle with stress and work-life balance',
        conversationContext: [
          'User mentioned they work in finance',
          'They have experience with other meditation apps',
          'Target is mobile-first experience'
        ],
        maxCriteria: 8
      };

      const result = await openaiService.generateDynamicChecklist(request);

      expect(result.success).toBe(true);
      expect(result.data).toBeDefined();
      expect(Array.isArray(result.data!.generatedCriteria)).toBe(true);
      expect(result.data!.generatedCriteria.length).toBe(2);
      expect(typeof result.data!.reasoning).toBe('string');
      expect(typeof result.data!.productType).toBe('string');
      expect(typeof result.data!.targetAudience).toBe('string');

      // Check criteria structure
      result.data!.generatedCriteria.forEach(criterion => {
        expect(typeof criterion.id).toBe('string');
        expect(typeof criterion.question).toBe('string');
        expect(typeof criterion.priority).toBe('number');
        expect(typeof criterion.reasoning).toBe('string');
        expect(criterion.priority).toBeGreaterThanOrEqual(1);
        expect(criterion.priority).toBeLessThanOrEqual(5);
      });
    });

    it('should limit criteria count to maxCriteria', async () => {
      console.log('ðŸ§ª Testing dynamic checklist criteria limiting');

      const longResponse = {
        ...mockDynamicChecklistResponse,
        choices: [{
          message: {
            content: JSON.stringify({
              generatedCriteria: Array(10).fill(null).map((_, i) => ({
                id: `criterion_${i + 1}`,
                question: `Question ${i + 1}?`,
                priority: 3,
                reasoning: `Reasoning ${i + 1}`
              })),
              reasoning: 'Many criteria generated',
              productType: 'Test Product',
              targetAudience: 'Test Users'
            })
          },
          finish_reason: 'stop'
        }]
      };

      mockFetch.mockResolvedValueOnce({
        ok: true,
        json: () => Promise.resolve(longResponse)
      });

      const request = {
        userIdea: 'E-commerce platform for handmade goods',
        maxCriteria: 5
      };

      const result = await openaiService.generateDynamicChecklist(request);

      expect(result.success).toBe(true);
      expect(result.data).toBeDefined();
      expect(result.data!.generatedCriteria.length).toBe(5); // Limited to max
    });

    it('should handle dynamic checklist generation errors gracefully', async () => {
      console.log('ðŸ§ª Testing dynamic checklist generation error handling');
      
      const mockErrorResponse = {
        ok: false,
        status: 500,
        statusText: 'Internal Server Error',
        json: vi.fn().mockResolvedValue({ 
          error: { 
            message: 'Internal server error occurred',
            type: 'server_error'
          }
        })
      };

      mockFetch.mockResolvedValue(mockErrorResponse);

      const result = await openaiService.generateDynamicChecklist({
        userIdea: 'Test idea that should trigger server error'
      });

      expect(result.success).toBe(false);
      expect(result.error).toContain('500');
      expect(result.error).toContain('Internal Server Error');
      expect(mockErrorResponse.json).toHaveBeenCalled();
    });

    it('should handle invalid JSON response', async () => {
      console.log('ðŸ§ª Testing dynamic checklist invalid JSON handling');

      const invalidJSONResponse = {
        ...mockDynamicChecklistResponse,
        choices: [{
          message: { content: 'Invalid JSON response from GPT-4' },
          finish_reason: 'stop'
        }]
      };

      mockFetch.mockResolvedValueOnce({
        ok: true,
        json: () => Promise.resolve(invalidJSONResponse)
      });

      const request = {
        userIdea: 'Test idea'
      };

      const result = await openaiService.generateDynamicChecklist(request);

      expect(result.success).toBe(false);
      expect(result.error).toContain('Invalid JSON response');
    });

    it('should handle missing generatedCriteria in response', async () => {
      console.log('ðŸ§ª Testing dynamic checklist missing criteria handling');

      const invalidResponse = {
        ...mockDynamicChecklistResponse,
        choices: [{
          message: {
            content: JSON.stringify({
              reasoning: 'Valid reasoning',
              productType: 'Test Product',
              targetAudience: 'Test Users'
              // Missing generatedCriteria
            })
          },
          finish_reason: 'stop'
        }]
      };

      mockFetch.mockResolvedValueOnce({
        ok: true,
        json: () => Promise.resolve(invalidResponse)
      });

      const request = {
        userIdea: 'Test idea'
      };

      const result = await openaiService.generateDynamicChecklist(request);

      expect(result.success).toBe(false);
      expect(result.error).toContain('missing or invalid generatedCriteria');
    });
  });

  describe('generatePartialAnswerFollowup', () => {
    const mockPartialFollowupRequest = {
      userResponse: 'My app is for busy professionals who need better productivity tools.',
      criterionId: 'target_audience',
      criterionQuestion: 'Who is your target audience and what are their specific characteristics?',
      previousAttempts: 0,
      conversationContext: ['Tell me about your app idea', 'My app is for busy professionals...'],
      operationId: 'test_partial_followup'
    };

    it('should successfully generate partial answer follow-up', async () => {
      console.log('ðŸ§ª Testing partial answer follow-up generation');
      
      const mockSuccessResponse = {
        ok: true,
        json: vi.fn().mockResolvedValue(mockPartialFollowupResponse)
      };

      mockFetch.mockResolvedValue(mockSuccessResponse);

      const result = await openaiService.generatePartialAnswerFollowup(mockPartialFollowupRequest);

      expect(result.success).toBe(true);
      expect(result.data).toBeDefined();
      expect(result.data?.followupQuestion).toContain('Yoda Says:');
      expect(result.data?.followupQuestion).toContain('professionals');
      expect(result.data?.shouldContinueProbing).toBe(true);
      expect(result.data?.reasoning).toContain('Yoda Says:');
      expect(result.data?.expectedInformation).toContain('specific industries');
      expect(mockSuccessResponse.json).toHaveBeenCalled();
      
      // Verify the API call was made correctly
      expect(mockFetch).toHaveBeenCalledWith(
        'https://api.openai.com/v1/chat/completions',
        expect.objectContaining({
          method: 'POST',
          headers: expect.objectContaining({
            'Content-Type': 'application/json',
            'Authorization': 'Bearer sk-test123456789abcdef'
          }),
          body: expect.stringContaining('target_audience')
        })
      );
    });

    it('should handle API errors gracefully', async () => {
      console.log('ðŸ§ª Testing partial answer follow-up API error handling');
      
      const mockErrorResponse = {
        ok: false,
        status: 429,
        statusText: 'Too Many Requests',
        json: vi.fn().mockResolvedValue({ 
          error: { 
            message: 'Rate limit exceeded for requests',
            type: 'rate_limit_error' 
          }
        })
      };

      mockFetch.mockResolvedValue(mockErrorResponse);

      const result = await openaiService.generatePartialAnswerFollowup(mockPartialFollowupRequest);

      expect(result.success).toBe(false);
      expect(result.error).toContain('429');
      expect(result.error).toContain('Too Many Requests');
      expect(mockErrorResponse.json).toHaveBeenCalled();
    });

    it('should handle should not probe response', async () => {
      console.log('ðŸ§ª Testing partial answer follow-up should not probe');
      
      const noProbeResponse = {
        choices: [
          {
            message: {
              content: JSON.stringify({
                followupQuestion: 'Yoda Says: I understand your target audience well. Let\'s explore another aspect.',
                shouldContinueProbing: false,
                reasoning: 'Yoda Says: The user provided sufficient detail about their target audience for initial validation.',
                expectedInformation: []
              })
            },
            finish_reason: 'stop'
          }
        ],
        usage: {
          prompt_tokens: 120,
          completion_tokens: 50,
          total_tokens: 170
        },
        model: 'gpt-4o'
      };

      mockFetch.mockResolvedValueOnce({
        ok: true,
        json: () => Promise.resolve(noProbeResponse)
      });

      const result = await openaiService.generatePartialAnswerFollowup(mockPartialFollowupRequest);

      expect(result.success).toBe(true);
      expect(result.data?.shouldContinueProbing).toBe(false);
      expect(result.data?.followupQuestion).toContain('Let\'s explore another aspect');
    });

    it('should handle invalid JSON response', async () => {
      console.log('ðŸ§ª Testing partial answer follow-up invalid JSON handling');
      
      const invalidJsonResponse = {
        choices: [
          { 
            message: { 
              content: 'Invalid JSON response here - not a proper JSON object' 
            },
            finish_reason: 'stop'
          }
        ],
        usage: {
          prompt_tokens: 100,
          completion_tokens: 20,
          total_tokens: 120
        },
        model: 'gpt-4o'
      };

      mockFetch.mockResolvedValueOnce({
        ok: true,
        json: () => Promise.resolve(invalidJsonResponse)
      });

      const result = await openaiService.generatePartialAnswerFollowup(mockPartialFollowupRequest);

      expect(result.success).toBe(false);
      expect(result.error).toContain('Invalid JSON');
    });
  });
});

/**
 * Integration test to verify GPT-4 is actually being called
 * Uses "Yoda Says:" prefix to confirm real GPT-4 responses
 */
describe('GPT-4 Integration Verification', () => {
  let openaiService: OpenAIService;

  beforeEach(() => {
    // Reset mocks for integration test
    vi.clearAllMocks();
    
    openaiService = new OpenAIService({
      apiKey: 'sk-test1234567890abcdef1234567890abcdef',
      baseUrl: 'https://api.openai.com/v1',
      timeout: 30000,
      maxRetries: 1,
      model: 'gpt-4o'
    });
  });

  it('should verify GPT-4 responses start with "Yoda Says:" for debugging', async () => {
    console.log('ðŸ§ª Testing GPT-4 integration with Yoda Says prefix');
    
    // Mock a simple text response that starts with "Yoda Says:"
    const mockYodaResponse = {
      choices: [
        {
          message: {
            content: 'Yoda Says: Great your lemon cake idea is! Tell me more about your target customers, you must.'
          },
          finish_reason: 'stop'
        }
      ],
      usage: {
        prompt_tokens: 50,
        completion_tokens: 20,
        total_tokens: 70
      },
      model: 'gpt-4o'
    };

    mockFetch.mockResolvedValueOnce({
      ok: true,
      json: () => Promise.resolve(mockYodaResponse)
    });

    // Create a simple GPT request
    const gptOptions: GPTRequestOptions = {
      systemPrompt: 'You are Master Yoda. Always start your responses with "Yoda Says:" and speak in Yoda\'s distinctive syntax. Be helpful but keep it short.',
      userMessage: 'I have an idea for a lemon cake business. What should I consider?',
      temperature: 0.7,
      maxTokens: 100
    };

    // Call the private method via type assertion for testing
    const result = await (openaiService as any).callGPT4(gptOptions, 'yoda_test_123');

    expect(result.success).toBe(true);
    expect(result.data?.content).toContain('Yoda Says:');
    expect(result.data?.content).toContain('lemon cake');
    expect(mockFetch).toHaveBeenCalledTimes(1);
    
    // Verify the actual API call structure
    const fetchCall = mockFetch.mock.calls[0];
    const requestBody = JSON.parse(fetchCall[1]?.body as string);
    
    expect(requestBody.model).toBe('gpt-4o');
    expect(requestBody.messages).toHaveLength(2); // system + user
    expect(requestBody.messages[0].role).toBe('system');
    expect(requestBody.messages[0].content).toContain('Yoda Says:');
    expect(requestBody.messages[1].role).toBe('user');
    expect(requestBody.messages[1].content).toContain('lemon cake');
    expect(requestBody.max_tokens).toBe(100);
    expect(requestBody.temperature).toBe(0.7);
  });

  it('should handle real-world GPT-4 JSON responses with markdown wrapping', async () => {
    console.log('ðŸ§ª Testing GPT-4 JSON parsing with markdown code blocks');
    
    // Mock a JSON response wrapped in markdown (like GPT-4 actually returns)
    const mockWrappedJsonResponse = {
      choices: [
        {
          message: {
            content: '```json\n{\n  "addressedCriteria": ["target_audience"],\n  "completionScores": {\n    "target_audience": 0.8,\n    "problem_definition": 0.2\n  },\n  "overallConfidence": 0.6,\n  "reasoning": "Yoda Says: Clear about your customers, you are. More about the problem they face, tell me you must."\n}\n```'
          },
          finish_reason: 'stop'
        }
      ],
      usage: {
        prompt_tokens: 100,
        completion_tokens: 80,
        total_tokens: 180
      },
      model: 'gpt-4o'
    };

    mockFetch.mockResolvedValueOnce({
      ok: true,
      json: () => Promise.resolve(mockWrappedJsonResponse)
    });

    const request: ValidationAnalysisRequest = {
      userResponse: 'My lemon cake is for busy professionals who want healthy desserts.',
      checklistCriteria: ['target_audience', 'problem_definition', 'key_features'],
      operationId: 'yoda_json_test_456'
    };

    const result = await openaiService.analyzeValidation(request);

    // This should succeed even with markdown-wrapped JSON
    expect(result.success).toBe(true);
    expect(result.data?.addressedCriteria).toContain('target_audience');
    expect(result.data?.reasoning).toContain('Yoda Says:');
    expect(result.data?.overallConfidence).toBe(0.6);
    expect(mockFetch).toHaveBeenCalledTimes(1);
  });
}); 